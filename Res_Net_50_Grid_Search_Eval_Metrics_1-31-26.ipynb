{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_cG8BwCZsJtm"
      },
      "outputs": [],
      "source": [
        "# Please use this to connect your GitHub repository to your Google Colab notebook\n",
        "# Connects to any needed files from GitHub and Google Drive\n",
        "\n",
        "# Remove Colab default sample_data\n",
        "# !rm -r ./sample_data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "from google.cloud import storage\n",
        "\n",
        "# Models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from torchvision.models import resnet50\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "# Absolute, stable local directory (fixes relative-path surprises)\n",
        "RAW_DIR = Path(\"./data/raw\").resolve()\n",
        "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(\"RAW_DIR:\", RAW_DIR)\n",
        "\n"
      ],
      "metadata": {
        "id": "ogvFG8nfRuFI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce508ea7-f180-4f30-a353-32df5cbf7179"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAW_DIR: /content/data/raw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Google Cloud Storage, a blob is just one stored object in a bucket — basically a file-like thing (an image, CSV, zip, etc.) plus its metadata.\n",
        "\n",
        "So in your code:\n",
        "\t•\tbucket.list_blobs(prefix=gcs_folder_prefix) returns an iterator of blob objects\n",
        "\t•\tEach blob corresponds to one object whose name starts with that prefix, e.g.\n",
        "imgs_folder/IXI050.png\n",
        "\t•\tblob.name is the object’s full path-like name inside the bucket\n",
        "\t•\tblob.download_to_filename(local_path) downloads that object’s bytes to your local file\n",
        "\n",
        "Important nuance: GCS doesn’t have real folders — “folders” are just name prefixes. So a “folder” like imgs_folder/ is really just “all blobs whose names start with imgs_folder/.”"
      ],
      "metadata": {
        "id": "zOPEhrFAraeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_gcs_folder(bucket_name, gcs_folder_prefix, local_dir):\n",
        "    \"\"\"\n",
        "    Downloads all blobs from a bucket with a specific prefix.\n",
        "    FLATTENS by filename (basename) to match your partners' logic,\n",
        "    but uses absolute local_dir and skips already-downloaded files.\n",
        "    \"\"\"\n",
        "    storage_client = storage.Client.create_anonymous_client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    if not gcs_folder_prefix.endswith('/'):\n",
        "        gcs_folder_prefix += '/'\n",
        "\n",
        "    blobs = bucket.list_blobs(prefix=gcs_folder_prefix)\n",
        "    print(f\"Searching for files in: gs://{bucket_name}/{gcs_folder_prefix}\")\n",
        "\n",
        "    downloaded = 0\n",
        "    for blob in blobs:\n",
        "        # Skip folder placeholders\n",
        "        if blob.name.endswith('/'):\n",
        "            continue\n",
        "\n",
        "        local_file_name = os.path.basename(blob.name)\n",
        "        if not local_file_name:\n",
        "            continue\n",
        "\n",
        "        local_path = Path(local_dir) / local_file_name\n",
        "        local_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Skip if already present\n",
        "        if local_path.exists():\n",
        "            continue\n",
        "\n",
        "        blob.download_to_filename(str(local_path))\n",
        "        downloaded += 1\n",
        "\n",
        "    print(f\"Done. Downloaded {downloaded} new files into: {Path(local_dir).resolve()}\")\n",
        "\n",
        "download_gcs_folder(\n",
        "    bucket_name='brain-age-mri-bucket',\n",
        "    gcs_folder_prefix='imgs_folder/',\n",
        "    local_dir=RAW_DIR\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOeMuB7c3gfM",
        "outputId": "0db154a9-2aa9-4346-c0af-eb9b18303ca9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for files in: gs://brain-age-mri-bucket/imgs_folder/\n",
            "Done. Downloaded 0 new files into: /content/data/raw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "vBKlPAEuSHFp",
        "outputId": "6fd888dc-8ace-44e6-e026-8f69555b48c1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download_gcs_folder(\n",
        "    bucket_name='brain-age-mri-bucket',\n",
        "    gcs_folder_prefix='imgs_folder/',\n",
        "    local_dir='./data/raw'\n",
        ")"
      ],
      "metadata": {
        "id": "Ykz_hlrsSDDY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d77afe1-b8e3-4c29-89ef-4f8bc3a7ec5d",
        "collapsed": true
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for files in: gs://brain-age-mri-bucket/imgs_folder/\n",
            "Done. Downloaded 0 new files into: /content/data/raw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0WvujpixsqBz",
        "outputId": "670b4ad5-498c-426b-b0cf-0aab9ce11005"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "EMtNpjyYE6I8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load + base selection\n",
        "df = pd.read_csv(\"./IXI_with_filenames.csv\")\n",
        "needed_df = df[[\"IXI_ID\", \"file_name\", \"AGE\", \"HEIGHT\", \"WEIGHT\"]].copy()  # edit cols if names differ\n",
        "\n",
        "# Drop rows missing core fields\n",
        "needed_df = needed_df.dropna(subset=[\"file_name\", \"AGE\"])\n",
        "\n",
        "# Coerce numeric for label + numeric features\n",
        "for col in [\"AGE\", \"HEIGHT\", \"WEIGHT\"]:\n",
        "    if col in needed_df.columns:\n",
        "        needed_df[col] = pd.to_numeric(needed_df[col], errors=\"coerce\")"
      ],
      "metadata": {
        "id": "uqNvOXZ23kUx"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# HEIGHT cleaning (cm)\n",
        "# -------------------------\n",
        "if \"HEIGHT\" in needed_df.columns:\n",
        "    # Treat 0 as missing\n",
        "    needed_df.loc[needed_df[\"HEIGHT\"] == 0, \"HEIGHT\"] = np.nan\n",
        "\n",
        "    # Fix likely \"cm*10\" entries (e.g., 1520 -> 152.0, 1850 -> 185.0)\n",
        "    mask_h_times10 = (needed_df[\"HEIGHT\"] > 300) & (needed_df[\"HEIGHT\"] < 2500)\n",
        "    needed_df.loc[mask_h_times10, \"HEIGHT\"] = needed_df.loc[mask_h_times10, \"HEIGHT\"] / 10.0\n",
        "\n",
        "    # Set impossible heights to NaN\n",
        "    H_MIN, H_MAX = 100, 210\n",
        "    needed_df.loc[(needed_df[\"HEIGHT\"] < H_MIN) | (needed_df[\"HEIGHT\"] > H_MAX), \"HEIGHT\"] = np.nan\n",
        "\n",
        "    # Impute with median\n",
        "    needed_df[\"HEIGHT\"] = needed_df[\"HEIGHT\"].fillna(needed_df[\"HEIGHT\"].median())"
      ],
      "metadata": {
        "id": "heZh43RVOnGs"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# WEIGHT cleaning (kg)\n",
        "# -------------------------\n",
        "if \"WEIGHT\" in needed_df.columns:\n",
        "    # Treat 0 as missing\n",
        "    needed_df.loc[needed_df[\"WEIGHT\"] == 0, \"WEIGHT\"] = np.nan\n",
        "\n",
        "    # Fix obvious \"kg*10\" outliers (e.g., 720 -> 72.0, 960 -> 96.0)\n",
        "    # Since your normal range is about 45-127 kgs, anything > 200 is almost certainly mis-scaled.\n",
        "    mask_w_times10 = (needed_df[\"WEIGHT\"] > 200) & (needed_df[\"WEIGHT\"] < 2000)\n",
        "    needed_df.loc[mask_w_times10, \"WEIGHT\"] = needed_df.loc[mask_w_times10, \"WEIGHT\"] / 10.0\n",
        "\n",
        "    # Set impossible weights to NaN (wide bounds, adjust if you want)\n",
        "    W_MIN, W_MAX = 35, 200\n",
        "    needed_df.loc[(needed_df[\"WEIGHT\"] < W_MIN) | (needed_df[\"WEIGHT\"] > W_MAX), \"WEIGHT\"] = np.nan\n",
        "\n",
        "    # Impute with median\n",
        "    needed_df[\"WEIGHT\"] = needed_df[\"WEIGHT\"].fillna(needed_df[\"WEIGHT\"].median())"
      ],
      "metadata": {
        "id": "EeVcghHMOqqm"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final sanity checks\n",
        "print(\"Shape:\", needed_df.shape)\n",
        "print(\"\\nHEIGHT summary:\\n\", needed_df[\"HEIGHT\"].describe() if \"HEIGHT\" in needed_df.columns else \"HEIGHT col not found\")\n",
        "print(\"\\nWEIGHT summary:\\n\", needed_df[\"WEIGHT\"].describe() if \"WEIGHT\" in needed_df.columns else \"WEIGHT col not found\")\n",
        "\n",
        "# Quick look at extremes (should look normal after cleaning)\n",
        "if \"HEIGHT\" in needed_df.columns:\n",
        "    print(\"\\nTop 10 HEIGHT:\", needed_df[\"HEIGHT\"].sort_values(ascending=False).head(10).to_list())\n",
        "    print(\"Bottom 10 HEIGHT:\", needed_df[\"HEIGHT\"].sort_values().head(10).to_list())\n",
        "\n",
        "if \"WEIGHT\" in needed_df.columns:\n",
        "    print(\"\\nTop 10 WEIGHT:\", needed_df[\"WEIGHT\"].sort_values(ascending=False).head(10).to_list())\n",
        "    print(\"Bottom 10 WEIGHT:\", needed_df[\"WEIGHT\"].sort_values().head(10).to_list())\n",
        "\n",
        "needed_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        },
        "id": "ux78TyphE8qv",
        "outputId": "ac988baf-e66a-4e3e-b80e-1eb72264b355"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (588, 5)\n",
            "\n",
            "HEIGHT summary:\n",
            " count    588.000000\n",
            "mean     169.724490\n",
            "std        9.824124\n",
            "min      110.000000\n",
            "25%      163.000000\n",
            "50%      170.000000\n",
            "75%      176.000000\n",
            "max      210.000000\n",
            "Name: HEIGHT, dtype: float64\n",
            "\n",
            "WEIGHT summary:\n",
            " count    588.000000\n",
            "mean      72.576531\n",
            "std       14.271614\n",
            "min       45.000000\n",
            "25%       62.000000\n",
            "50%       70.000000\n",
            "75%       80.000000\n",
            "max      127.000000\n",
            "Name: WEIGHT, dtype: float64\n",
            "\n",
            "Top 10 HEIGHT: [210.0, 203.0, 195.0, 195.0, 194.0, 193.0, 192.0, 190.0, 190.0, 190.0]\n",
            "Bottom 10 HEIGHT: [110.0, 143.0, 149.0, 150.0, 150.0, 150.0, 150.0, 151.0, 151.0, 152.0]\n",
            "\n",
            "Top 10 WEIGHT: [127.0, 123.0, 123.0, 121.0, 120.0, 116.0, 115.0, 114.0, 114.0, 112.0]\n",
            "Bottom 10 WEIGHT: [45.0, 47.0, 47.0, 49.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   IXI_ID                file_name        AGE  HEIGHT  WEIGHT\n",
              "1       2  IXI002-Guys-0828-T1.png  35.800137   164.0    58.0\n",
              "2      12    IXI012-HH-1211-T1.png  38.781656   175.0    70.0\n",
              "3      13    IXI013-HH-1212-T1.png  46.710472   182.0    70.0\n",
              "4      14    IXI014-HH-1236-T1.png  34.236824   163.0    65.0\n",
              "5      15    IXI015-HH-1258-T1.png  24.284736   181.0    90.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-be83b80d-83f1-4efe-88ed-cf5589050968\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>IXI_ID</th>\n",
              "      <th>file_name</th>\n",
              "      <th>AGE</th>\n",
              "      <th>HEIGHT</th>\n",
              "      <th>WEIGHT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>IXI002-Guys-0828-T1.png</td>\n",
              "      <td>35.800137</td>\n",
              "      <td>164.0</td>\n",
              "      <td>58.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12</td>\n",
              "      <td>IXI012-HH-1211-T1.png</td>\n",
              "      <td>38.781656</td>\n",
              "      <td>175.0</td>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13</td>\n",
              "      <td>IXI013-HH-1212-T1.png</td>\n",
              "      <td>46.710472</td>\n",
              "      <td>182.0</td>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14</td>\n",
              "      <td>IXI014-HH-1236-T1.png</td>\n",
              "      <td>34.236824</td>\n",
              "      <td>163.0</td>\n",
              "      <td>65.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>15</td>\n",
              "      <td>IXI015-HH-1258-T1.png</td>\n",
              "      <td>24.284736</td>\n",
              "      <td>181.0</td>\n",
              "      <td>90.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be83b80d-83f1-4efe-88ed-cf5589050968')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-be83b80d-83f1-4efe-88ed-cf5589050968 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-be83b80d-83f1-4efe-88ed-cf5589050968');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "needed_df",
              "summary": "{\n  \"name\": \"needed_df\",\n  \"rows\": 588,\n  \"fields\": [\n    {\n      \"column\": \"IXI_ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 184,\n        \"min\": 2,\n        \"max\": 662,\n        \"num_unique_values\": 563,\n        \"samples\": [\n          290,\n          599,\n          309\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"file_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 563,\n        \"samples\": [\n          \"IXI290-IOP-0874-T1.png\",\n          \"IXI599-HH-2659-T1.png\",\n          \"IXI309-IOP-0897-T1.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AGE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16.697722698342574,\n        \"min\": 19.980835044490075,\n        \"max\": 86.3189596167009,\n        \"num_unique_values\": 541,\n        \"samples\": [\n          61.06502395619439,\n          28.61327857631759,\n          46.29705681040383\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HEIGHT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9.8241235046863,\n        \"min\": 110.0,\n        \"max\": 210.0,\n        \"num_unique_values\": 50,\n        \"samples\": [\n          167.0,\n          195.0,\n          169.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"WEIGHT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.27161417807331,\n        \"min\": 45.0,\n        \"max\": 127.0,\n        \"num_unique_values\": 69,\n        \"samples\": [\n          53.0,\n          58.0,\n          96.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48eb91c5"
      },
      "source": [
        "# Task\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "602de354"
      },
      "source": [
        "## Prepare Data Paths\n",
        "\n",
        "### Subtask:\n",
        "Create a new column in `needed_df` with the full paths to the image files, combining the `/data/raw` directory with the 'file_name' column.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dc1a523"
      },
      "source": [
        "**Reasoning**:\n",
        "To create the 'image_path' column, I will combine the directory '/data/raw' with the 'file_name' using `os.path.join` and the `apply` method on the 'file_name' column, then display the first few rows to verify.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4839939e"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code generated a `SettingWithCopyWarning`. To avoid this warning and ensure proper DataFrame modification, I will use `.loc` for assigning the new 'image_path' column to `needed_df`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: normalize filenames so they match how download_gcs_folder saved them (basename)\n",
        "needed_df = needed_df.copy()\n",
        "needed_df[\"file_name\"] = (\n",
        "    needed_df[\"file_name\"]\n",
        "    .astype(str)\n",
        "    .str.strip()\n",
        "    .apply(lambda x: os.path.basename(x))\n",
        ")\n",
        "\n",
        "needed_df[\"image_path\"] = needed_df[\"file_name\"].apply(lambda x: str(RAW_DIR / x))\n",
        "needed_df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "sda5Slrf30tz",
        "outputId": "90902e04-cc56-4c77-a5ae-39d3b6af8070"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   IXI_ID                file_name        AGE  HEIGHT  WEIGHT  \\\n",
              "1       2  IXI002-Guys-0828-T1.png  35.800137   164.0    58.0   \n",
              "2      12    IXI012-HH-1211-T1.png  38.781656   175.0    70.0   \n",
              "3      13    IXI013-HH-1212-T1.png  46.710472   182.0    70.0   \n",
              "4      14    IXI014-HH-1236-T1.png  34.236824   163.0    65.0   \n",
              "5      15    IXI015-HH-1258-T1.png  24.284736   181.0    90.0   \n",
              "\n",
              "                                  image_path  \n",
              "1  /content/data/raw/IXI002-Guys-0828-T1.png  \n",
              "2    /content/data/raw/IXI012-HH-1211-T1.png  \n",
              "3    /content/data/raw/IXI013-HH-1212-T1.png  \n",
              "4    /content/data/raw/IXI014-HH-1236-T1.png  \n",
              "5    /content/data/raw/IXI015-HH-1258-T1.png  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-845c248e-da01-4ba6-ba82-0721db0dfc6d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>IXI_ID</th>\n",
              "      <th>file_name</th>\n",
              "      <th>AGE</th>\n",
              "      <th>HEIGHT</th>\n",
              "      <th>WEIGHT</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>IXI002-Guys-0828-T1.png</td>\n",
              "      <td>35.800137</td>\n",
              "      <td>164.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>/content/data/raw/IXI002-Guys-0828-T1.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12</td>\n",
              "      <td>IXI012-HH-1211-T1.png</td>\n",
              "      <td>38.781656</td>\n",
              "      <td>175.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>/content/data/raw/IXI012-HH-1211-T1.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13</td>\n",
              "      <td>IXI013-HH-1212-T1.png</td>\n",
              "      <td>46.710472</td>\n",
              "      <td>182.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>/content/data/raw/IXI013-HH-1212-T1.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14</td>\n",
              "      <td>IXI014-HH-1236-T1.png</td>\n",
              "      <td>34.236824</td>\n",
              "      <td>163.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>/content/data/raw/IXI014-HH-1236-T1.png</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>15</td>\n",
              "      <td>IXI015-HH-1258-T1.png</td>\n",
              "      <td>24.284736</td>\n",
              "      <td>181.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>/content/data/raw/IXI015-HH-1258-T1.png</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-845c248e-da01-4ba6-ba82-0721db0dfc6d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-845c248e-da01-4ba6-ba82-0721db0dfc6d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-845c248e-da01-4ba6-ba82-0721db0dfc6d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "needed_df",
              "summary": "{\n  \"name\": \"needed_df\",\n  \"rows\": 588,\n  \"fields\": [\n    {\n      \"column\": \"IXI_ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 184,\n        \"min\": 2,\n        \"max\": 662,\n        \"num_unique_values\": 563,\n        \"samples\": [\n          290,\n          599,\n          309\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"file_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 563,\n        \"samples\": [\n          \"IXI290-IOP-0874-T1.png\",\n          \"IXI599-HH-2659-T1.png\",\n          \"IXI309-IOP-0897-T1.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AGE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16.697722698342574,\n        \"min\": 19.980835044490075,\n        \"max\": 86.3189596167009,\n        \"num_unique_values\": 541,\n        \"samples\": [\n          61.06502395619439,\n          28.61327857631759,\n          46.29705681040383\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HEIGHT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9.8241235046863,\n        \"min\": 110.0,\n        \"max\": 210.0,\n        \"num_unique_values\": 50,\n        \"samples\": [\n          167.0,\n          195.0,\n          169.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"WEIGHT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.27161417807331,\n        \"min\": 45.0,\n        \"max\": 127.0,\n        \"num_unique_values\": 69,\n        \"samples\": [\n          53.0,\n          58.0,\n          96.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 563,\n        \"samples\": [\n          \"/content/data/raw/IXI290-IOP-0874-T1.png\",\n          \"/content/data/raw/IXI599-HH-2659-T1.png\",\n          \"/content/data/raw/IXI309-IOP-0897-T1.png\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exists_mask = needed_df[\"image_path\"].apply(os.path.exists)\n",
        "missing_df = needed_df.loc[~exists_mask, [\"IXI_ID\", \"file_name\", \"image_path\"]]\n",
        "\n",
        "print(\"Images found locally:\", int(exists_mask.sum()))\n",
        "print(\"Images missing locally:\", int((~exists_mask).sum()))\n",
        "\n",
        "if len(missing_df) > 0:\n",
        "    print(\"\\nSample missing (first 10):\")\n",
        "    print(missing_df.head(10).to_string(index=False))\n",
        "\n",
        "# Drop still-missing rows so DataLoader won't crash\n",
        "needed_df = needed_df.loc[exists_mask].reset_index(drop=True)\n",
        "print(\"Final usable rows:\", len(needed_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6Za4_B439Al",
        "outputId": "37b5e15e-542e-4f1c-b129-67bedaa2bca3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images found locally: 588\n",
            "Images missing locally: 0\n",
            "Final usable rows: 588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cf9a464"
      },
      "source": [
        "## Split Data\n",
        "\n",
        "### Subtask:\n",
        "Split the `needed_df` into training and validation sets to prepare for model training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dc65b86"
      },
      "source": [
        "**Reasoning**:\n",
        "To split the data, I will import the `train_test_split` function, define features and target variables, and then use the function to create training and validation sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52cd27e8",
        "outputId": "0b9466eb-69df-4a5b-ae93-619f1bfd0910"
      },
      "source": [
        "X = needed_df['image_path']\n",
        "y = needed_df['AGE']\n",
        "\n",
        "# First, split into training and a combined validation/test set: 70% train, 30% val & test\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Then, split the combined validation/test set into validation and test sets: 70$ train, 15% val & 15% test\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42) # 0.5 of 0.3 = 0.15 of original\n",
        "\n",
        "print(f\"Training set size: {len(X_train)} samples\")\n",
        "print(f\"Validation set size: {len(X_val)} samples\")\n",
        "print(f\"Test set size: {len(X_test)} samples\")\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 411 samples\n",
            "Validation set size: 88 samples\n",
            "Test set size: 89 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dbda12f"
      },
      "source": [
        "## Create PyTorch Dataset and DataLoader\n",
        "\n",
        "### Subtask:\n",
        "Define a custom PyTorch `Dataset` to handle image loading, transformations (resizing, normalization), and age label retrieval. Then, create `DataLoader` instances for both training and validation sets for efficient batch processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "663a54fb"
      },
      "source": [
        "**Reasoning**:\n",
        "To begin creating the custom PyTorch Dataset and DataLoader, the first step is to import all the necessary libraries and modules that will be used for data handling, transformations, and batch loading.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c535de14",
        "outputId": "addc2a2d-70a3-43ac-d010-928b77baa25e"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "print(\"PyTorch modules imported.\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch modules imported.\n",
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c020503"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the necessary modules are imported, I will define the custom PyTorch `Dataset` class, `MRIImageDataset`, to handle image loading, transformations, and age label retrieval. This will include the `__init__`, `__len__`, and `__getitem__` methods, as well as the image transformations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d167e772"
      },
      "source": [
        "## Load and Modify ResNet50\n",
        "\n",
        "### Subtask:\n",
        "Load the pre-trained ResNet50 model from `torchvision.models`. Modify its final fully connected layer to output a single numerical value, suitable for age regression.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04d5dbea"
      },
      "source": [
        "**Reasoning**:\n",
        "To begin, I will import the necessary `resnet50` function from `torchvision.models` and `torch.nn` for model definition. Then, I will load the pre-trained ResNet50 model and modify its final fully connected layer for age regression, before printing the model's architecture to confirm the changes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Backbone: ResNet-50 (Pre-trained on ImageNet):\n",
        "\n",
        "\t•\tInput: a 3-channel image goes through many convolution and “skip-connection” blocks (4 main stages)\n",
        "\n",
        "\t•\tOutput of the Backbone: the image is summarized into a fixed set of 2048 numbers that represent the learned image features\n",
        "\n",
        "\t•\tChange we Made: replace the original classifier with one final linear layer that outputs a single number (the predicted age)\n",
        "\n",
        "\t•\tTraining Strategy: keep the pretrained backbone fixed and train only the final layer\n",
        "\n",
        "#### What Changed vs. Original ResNet-50:\n",
        "\n",
        "\t•\tOriginal: final layer outputs 1000 class scores for ImageNet categories (then chooses a category)\n",
        "  \n",
        "\t•\tOurs: final layer outputs 1 number for age (a continuous prediction)"
      ],
      "metadata": {
        "id": "Cy8QemihhaCY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c536c82",
        "outputId": "bd1d7c89-766b-4ed6-9d6c-6dbbb305e208",
        "collapsed": true
      },
      "source": [
        "from torchvision.models import resnet50\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "model = resnet50(pretrained=True)\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 1)\n",
        "\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"Modified ResNet50 model architecture:\")\n",
        "print(model)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modified ResNet50 model architecture:\n",
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e704c08b"
      },
      "source": [
        "## Define Loss Function and Optimizer\n",
        "\n",
        "### Subtask:\n",
        "Mean Squared Error (MSE) as the loss function\n",
        "\n",
        "Adam optimizer to train the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b1cf70a"
      },
      "source": [
        "**Reasoning**:\n",
        "To define the loss function and optimizer, I will first determine the appropriate device (GPU or CPU) for computation, move the model to that device, and then instantiate the Mean Squared Error loss function and the Adam optimizer with a specified learning rate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e912d80b"
      },
      "source": [
        "## Train Model\n",
        "\n",
        "### Subtask:\n",
        "Implement and execute the training loop, including forward pass, loss calculation, backpropagation, and optimizer steps, over multiple epochs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7564c7f2"
      },
      "source": [
        "**Reasoning**:\n",
        "To implement the training loop as instructed, I will define the number of epochs, initialize lists for storing losses, and then write the nested loops for training and validation, including forward pass, loss calculation, backpropagation, and optimizer steps, while ensuring proper device placement and model mode settings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9e62382"
      },
      "source": [
        "class AddGaussianNoise(object):\n",
        "    def __init__(self, mean=0., std=1.):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
        "\n",
        "class MRIImageDataset(Dataset):\n",
        "    def __init__(self, image_paths, ages, transform=None):\n",
        "        self.image_paths = image_paths.astype(str).to_numpy()\n",
        "        self.ages = ages.to_numpy(dtype=np.float32)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "\n",
        "        if not os.path.exists(img_path):\n",
        "            raise FileNotFoundError(f\"Missing image on disk: {img_path}\")\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        age = self.ages[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(age, dtype=torch.float32)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca55f0c1"
      },
      "source": [
        "**Reasoning**:\n",
        "Since the `MRIImageDataset` and the transformations have been updated to correctly handle 3-channel input, the next step is to re-create the `DataLoader` instances with these updated definitions. After that, I will re-execute the training and validation loops to apply the corrected data processing and ensure the model receives appropriate input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10, ),\n",
        "    transforms.GaussianBlur(kernel_size=(5, 9)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    AddGaussianNoise(0., 0.1),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "print(\"MRIImageDataset class and transformations updated for RGB input, with separate transforms for validation and test sets.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0MJErfm4pfV",
        "outputId": "4e76c48d-1ab1-42e1-c5d8-6b620e89777d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MRIImageDataset class and transformations updated for RGB input, with separate transforms for validation and test sets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "num_workers = 0  # start at 0 to debug; switch back to 2 after it works\n",
        "\n",
        "train_dataset = MRIImageDataset(X_train, y_train, transform=train_transform)\n",
        "val_dataset = MRIImageDataset(X_val, y_val, transform=val_transform)\n",
        "test_dataset = MRIImageDataset(X_test, y_test, transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "print(f\"Training DataLoader created with {len(train_dataset)} samples and batch size {batch_size}.\")\n",
        "print(f\"Validation DataLoader created with {len(val_dataset)} samples and batch size {batch_size}.\")\n",
        "print(f\"Test DataLoader created with {len(test_dataset)} samples and batch size {batch_size}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rspHMFWK4v7p",
        "outputId": "95844075-3cea-450c-959c-c3f49ed3e9d2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training DataLoader created with 411 samples and batch size 32.\n",
            "Validation DataLoader created with 88 samples and batch size 32.\n",
            "Test DataLoader created with 89 samples and batch size 32.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import itertools\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ----------------------------\n",
        "# Grid Search (minimal-change)\n",
        "# Tunes: batch_size, optimizer, lr, weight_decay, unfreeze_last_block\n",
        "# ----------------------------\n",
        "\n",
        "# Save initial model weights so every trial starts identically\n",
        "base_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "# Store your \"base\" num_workers so we can reuse it\n",
        "BASE_NUM_WORKERS = num_workers\n",
        "\n",
        "# Keep this small so search is fast; after picking best cfg, Chunk 14 does full training\n",
        "search_epochs = 3\n",
        "\n",
        "# --- Grid (edit these) ---\n",
        "batch_size_grid = [16, 32, 64]\n",
        "optimizer_grid = [\"adam\", \"adamw\"]\n",
        "lr_grid = [1e-4, 3e-4, 1e-3]\n",
        "wd_grid = [0.0, 1e-5, 1e-4]\n",
        "unfreeze_last_block_grid = [False, True]  # False = train only head; True = train head + layer4\n",
        "\n",
        "def set_trainable(unfreeze_last_block: bool):\n",
        "    # Freeze everything\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # Always train the regression head\n",
        "    for p in model.fc.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    # Optionally unfreeze last ResNet stage (layer4)\n",
        "    if unfreeze_last_block:\n",
        "        for p in model.layer4.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "def make_optimizer(opt_name: str, lr: float, wd: float):\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    if opt_name == \"adamw\":\n",
        "        return optim.AdamW(params, lr=lr, weight_decay=wd)\n",
        "    return optim.Adam(params, lr=lr, weight_decay=wd)\n",
        "\n",
        "def build_trial_loaders(bs: int):\n",
        "    # Rebuild loaders with a different batch size (datasets stay the same)\n",
        "    train_loader_t = DataLoader(train_dataset, batch_size=bs, shuffle=True,  num_workers=BASE_NUM_WORKERS)\n",
        "    val_loader_t   = DataLoader(val_dataset,   batch_size=bs, shuffle=False, num_workers=BASE_NUM_WORKERS)\n",
        "    test_loader_t  = DataLoader(test_dataset,  batch_size=bs, shuffle=False, num_workers=BASE_NUM_WORKERS)\n",
        "    return train_loader_t, val_loader_t, test_loader_t\n",
        "\n",
        "def run_trial(cfg):\n",
        "    # Reset weights\n",
        "    model.load_state_dict(base_state)\n",
        "\n",
        "    # Set which params are trainable\n",
        "    set_trainable(cfg[\"unfreeze_last_block\"])\n",
        "\n",
        "    # Use smaller lr when unfreezing layer4 (usually safer)\n",
        "    opt = make_optimizer(cfg[\"optimizer\"], cfg[\"lr\"], cfg[\"weight_decay\"])\n",
        "\n",
        "    # Rebuild loaders for this batch size\n",
        "    train_loader_t, val_loader_t, _ = build_trial_loaders(cfg[\"batch_size\"])\n",
        "\n",
        "    # Train/val for a few epochs\n",
        "    for _ in range(search_epochs):\n",
        "        model.train()\n",
        "        for images, ages in train_loader_t:\n",
        "            images = images.to(device)\n",
        "            ages = ages.to(device)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            outputs = model(images).squeeze(1)  # (B,1)->(B,)\n",
        "            loss = criterion(outputs, ages)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss_sum = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, ages in val_loader_t:\n",
        "                images = images.to(device)\n",
        "                ages = ages.to(device)\n",
        "\n",
        "                outputs = model(images).squeeze(1)\n",
        "                loss = criterion(outputs, ages)\n",
        "                val_loss_sum += loss.item() * images.size(0)\n",
        "\n",
        "        val_loss = val_loss_sum / len(val_dataset)\n",
        "\n",
        "    return float(val_loss)\n",
        "\n",
        "# If you want speed + fewer surprises while tuning, force num_workers=0 during the search:\n",
        "# (Then restore after)\n",
        "orig_num_workers = BASE_NUM_WORKERS\n",
        "# BASE_NUM_WORKERS = 0\n",
        "\n",
        "configs = []\n",
        "for bs, opt_name, lr, wd, unf in itertools.product(\n",
        "    batch_size_grid, optimizer_grid, lr_grid, wd_grid, unfreeze_last_block_grid\n",
        "):\n",
        "    # Optional safety: when unfreezing layer4, avoid huge lrs\n",
        "    if unf and lr > 1e-3:\n",
        "        continue\n",
        "    configs.append({\n",
        "        \"batch_size\": bs,\n",
        "        \"optimizer\": opt_name,\n",
        "        \"lr\": lr,\n",
        "        \"weight_decay\": wd,\n",
        "        \"unfreeze_last_block\": unf\n",
        "    })\n",
        "\n",
        "print(f\"\\n--- Grid search over {len(configs)} configs (search_epochs={search_epochs}) ---\")\n",
        "\n",
        "best_cfg = None\n",
        "best_val = float(\"inf\")\n",
        "\n",
        "for i, cfg in enumerate(configs, 1):\n",
        "    val_loss = run_trial(cfg)\n",
        "    print(\n",
        "        f\"[{i:02d}/{len(configs)}] \"\n",
        "        f\"bs={cfg['batch_size']}, opt={cfg['optimizer']}, lr={cfg['lr']:.1e}, wd={cfg['weight_decay']:.1e}, \"\n",
        "        f\"unfreeze_layer4={cfg['unfreeze_last_block']} -> val_loss={val_loss:.4f}\"\n",
        "    )\n",
        "\n",
        "    if val_loss < best_val:\n",
        "        best_val = val_loss\n",
        "        best_cfg = cfg\n",
        "\n",
        "print(\"\\nBEST CONFIG:\", best_cfg, \"best_val_loss=\", round(best_val, 4))\n",
        "\n",
        "# ----------------------------\n",
        "# Apply BEST config for your REAL training (Chunk 14 unchanged)\n",
        "# ----------------------------\n",
        "\n",
        "# Reset to base weights before full training\n",
        "model.load_state_dict(base_state)\n",
        "\n",
        "# Set trainable params according to best config\n",
        "set_trainable(best_cfg[\"unfreeze_last_block\"])\n",
        "\n",
        "# Overwrite batch_size + loaders to the best batch size\n",
        "batch_size = best_cfg[\"batch_size\"]\n",
        "train_loader, val_loader, test_loader = build_trial_loaders(batch_size)\n",
        "\n",
        "# Overwrite optimizer to best choice\n",
        "optimizer = make_optimizer(best_cfg[\"optimizer\"], best_cfg[\"lr\"], best_cfg[\"weight_decay\"])\n",
        "\n",
        "print(f\"\\nUsing for full training (Chunk 14): batch_size={batch_size}, optimizer={best_cfg['optimizer']}, \"\n",
        "      f\"lr={best_cfg['lr']}, wd={best_cfg['weight_decay']}, unfreeze_layer4={best_cfg['unfreeze_last_block']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9l4cLXz-P55",
        "outputId": "8f718693-475f-4c76-8866-71d66c06bd71"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Grid search over 108 configs (search_epochs=3) ---\n",
            "[01/108] bs=16, opt=adam, lr=1.0e-04, wd=0.0e+00, unfreeze_layer4=False -> val_loss=17212.3277\n",
            "[02/108] bs=16, opt=adam, lr=1.0e-04, wd=0.0e+00, unfreeze_layer4=True -> val_loss=14526.9750\n",
            "[03/108] bs=16, opt=adam, lr=1.0e-04, wd=1.0e-05, unfreeze_layer4=False -> val_loss=16141.1179\n",
            "[04/108] bs=16, opt=adam, lr=1.0e-04, wd=1.0e-05, unfreeze_layer4=True -> val_loss=7965.7937\n",
            "[05/108] bs=16, opt=adam, lr=1.0e-04, wd=1.0e-04, unfreeze_layer4=False -> val_loss=17124.7631\n",
            "[06/108] bs=16, opt=adam, lr=1.0e-04, wd=1.0e-04, unfreeze_layer4=True -> val_loss=11949.4119\n",
            "[07/108] bs=16, opt=adam, lr=3.0e-04, wd=0.0e+00, unfreeze_layer4=False -> val_loss=16686.2905\n",
            "[08/108] bs=16, opt=adam, lr=3.0e-04, wd=0.0e+00, unfreeze_layer4=True -> val_loss=6787.9023\n",
            "[09/108] bs=16, opt=adam, lr=3.0e-04, wd=1.0e-05, unfreeze_layer4=False -> val_loss=18343.6182\n",
            "[10/108] bs=16, opt=adam, lr=3.0e-04, wd=1.0e-05, unfreeze_layer4=True -> val_loss=5268.1017\n",
            "[11/108] bs=16, opt=adam, lr=3.0e-04, wd=1.0e-04, unfreeze_layer4=False -> val_loss=18261.5841\n",
            "[12/108] bs=16, opt=adam, lr=3.0e-04, wd=1.0e-04, unfreeze_layer4=True -> val_loss=11375.3263\n",
            "[13/108] bs=16, opt=adam, lr=1.0e-03, wd=0.0e+00, unfreeze_layer4=False -> val_loss=15785.1165\n",
            "[14/108] bs=16, opt=adam, lr=1.0e-03, wd=0.0e+00, unfreeze_layer4=True -> val_loss=1399.5423\n",
            "[15/108] bs=16, opt=adam, lr=1.0e-03, wd=1.0e-05, unfreeze_layer4=False -> val_loss=15892.9661\n",
            "[16/108] bs=16, opt=adam, lr=1.0e-03, wd=1.0e-05, unfreeze_layer4=True -> val_loss=781.4480\n",
            "[17/108] bs=16, opt=adam, lr=1.0e-03, wd=1.0e-04, unfreeze_layer4=False -> val_loss=16303.4933\n",
            "[18/108] bs=16, opt=adam, lr=1.0e-03, wd=1.0e-04, unfreeze_layer4=True -> val_loss=1101.1763\n",
            "[19/108] bs=16, opt=adamw, lr=1.0e-04, wd=0.0e+00, unfreeze_layer4=False -> val_loss=17636.0856\n",
            "[20/108] bs=16, opt=adamw, lr=1.0e-04, wd=0.0e+00, unfreeze_layer4=True -> val_loss=4625.4673\n",
            "[21/108] bs=16, opt=adamw, lr=1.0e-04, wd=1.0e-05, unfreeze_layer4=False -> val_loss=16944.1800\n",
            "[22/108] bs=16, opt=adamw, lr=1.0e-04, wd=1.0e-05, unfreeze_layer4=True -> val_loss=8828.1091\n",
            "[23/108] bs=16, opt=adamw, lr=1.0e-04, wd=1.0e-04, unfreeze_layer4=False -> val_loss=17878.8480\n",
            "[24/108] bs=16, opt=adamw, lr=1.0e-04, wd=1.0e-04, unfreeze_layer4=True -> val_loss=17172.9990\n",
            "[25/108] bs=16, opt=adamw, lr=3.0e-04, wd=0.0e+00, unfreeze_layer4=False -> val_loss=17000.8340\n",
            "[26/108] bs=16, opt=adamw, lr=3.0e-04, wd=0.0e+00, unfreeze_layer4=True -> val_loss=4198.5756\n",
            "[27/108] bs=16, opt=adamw, lr=3.0e-04, wd=1.0e-05, unfreeze_layer4=False -> val_loss=16143.2650\n",
            "[28/108] bs=16, opt=adamw, lr=3.0e-04, wd=1.0e-05, unfreeze_layer4=True -> val_loss=6587.3800\n",
            "[29/108] bs=16, opt=adamw, lr=3.0e-04, wd=1.0e-04, unfreeze_layer4=False -> val_loss=16960.2991\n",
            "[30/108] bs=16, opt=adamw, lr=3.0e-04, wd=1.0e-04, unfreeze_layer4=True -> val_loss=7283.5415\n",
            "[31/108] bs=16, opt=adamw, lr=1.0e-03, wd=0.0e+00, unfreeze_layer4=False -> val_loss=14863.0188\n",
            "[32/108] bs=16, opt=adamw, lr=1.0e-03, wd=0.0e+00, unfreeze_layer4=True -> val_loss=1374.9738\n",
            "[33/108] bs=16, opt=adamw, lr=1.0e-03, wd=1.0e-05, unfreeze_layer4=False -> val_loss=16902.7234\n",
            "[34/108] bs=16, opt=adamw, lr=1.0e-03, wd=1.0e-05, unfreeze_layer4=True -> val_loss=322.5186\n",
            "[35/108] bs=16, opt=adamw, lr=1.0e-03, wd=1.0e-04, unfreeze_layer4=False -> val_loss=16154.6452\n",
            "[36/108] bs=16, opt=adamw, lr=1.0e-03, wd=1.0e-04, unfreeze_layer4=True -> val_loss=3638.3941\n",
            "[37/108] bs=32, opt=adam, lr=1.0e-04, wd=0.0e+00, unfreeze_layer4=False -> val_loss=16623.5590\n",
            "[38/108] bs=32, opt=adam, lr=1.0e-04, wd=0.0e+00, unfreeze_layer4=True -> val_loss=21705.7896\n",
            "[39/108] bs=32, opt=adam, lr=1.0e-04, wd=1.0e-05, unfreeze_layer4=False -> val_loss=17644.0112\n",
            "[40/108] bs=32, opt=adam, lr=1.0e-04, wd=1.0e-05, unfreeze_layer4=True -> val_loss=8180.6431\n",
            "[41/108] bs=32, opt=adam, lr=1.0e-04, wd=1.0e-04, unfreeze_layer4=False -> val_loss=18257.5545\n",
            "[42/108] bs=32, opt=adam, lr=1.0e-04, wd=1.0e-04, unfreeze_layer4=True -> val_loss=17904.5884\n",
            "[43/108] bs=32, opt=adam, lr=3.0e-04, wd=0.0e+00, unfreeze_layer4=False -> val_loss=17537.9577\n",
            "[44/108] bs=32, opt=adam, lr=3.0e-04, wd=0.0e+00, unfreeze_layer4=True -> val_loss=21638.0953\n",
            "[45/108] bs=32, opt=adam, lr=3.0e-04, wd=1.0e-05, unfreeze_layer4=False -> val_loss=16071.5896\n",
            "[46/108] bs=32, opt=adam, lr=3.0e-04, wd=1.0e-05, unfreeze_layer4=True -> val_loss=10558.6857\n",
            "[47/108] bs=32, opt=adam, lr=3.0e-04, wd=1.0e-04, unfreeze_layer4=False -> val_loss=16882.8462\n",
            "[48/108] bs=32, opt=adam, lr=3.0e-04, wd=1.0e-04, unfreeze_layer4=True -> val_loss=15382.2455\n",
            "[49/108] bs=32, opt=adam, lr=1.0e-03, wd=0.0e+00, unfreeze_layer4=False -> val_loss=16586.7349\n",
            "[50/108] bs=32, opt=adam, lr=1.0e-03, wd=0.0e+00, unfreeze_layer4=True -> val_loss=2316.4749\n",
            "[51/108] bs=32, opt=adam, lr=1.0e-03, wd=1.0e-05, unfreeze_layer4=False -> val_loss=17332.3375\n",
            "[52/108] bs=32, opt=adam, lr=1.0e-03, wd=1.0e-05, unfreeze_layer4=True -> val_loss=745.6816\n",
            "[53/108] bs=32, opt=adam, lr=1.0e-03, wd=1.0e-04, unfreeze_layer4=False -> val_loss=18442.4208\n",
            "[54/108] bs=32, opt=adam, lr=1.0e-03, wd=1.0e-04, unfreeze_layer4=True -> val_loss=3067.4788\n",
            "[55/108] bs=32, opt=adamw, lr=1.0e-04, wd=0.0e+00, unfreeze_layer4=False -> val_loss=17576.5733\n",
            "[56/108] bs=32, opt=adamw, lr=1.0e-04, wd=0.0e+00, unfreeze_layer4=True -> val_loss=5240.5438\n",
            "[57/108] bs=32, opt=adamw, lr=1.0e-04, wd=1.0e-05, unfreeze_layer4=False -> val_loss=17029.7551\n",
            "[58/108] bs=32, opt=adamw, lr=1.0e-04, wd=1.0e-05, unfreeze_layer4=True -> val_loss=12001.6800\n",
            "[59/108] bs=32, opt=adamw, lr=1.0e-04, wd=1.0e-04, unfreeze_layer4=False -> val_loss=18399.8018\n",
            "[60/108] bs=32, opt=adamw, lr=1.0e-04, wd=1.0e-04, unfreeze_layer4=True -> val_loss=11029.4285\n",
            "[61/108] bs=32, opt=adamw, lr=3.0e-04, wd=0.0e+00, unfreeze_layer4=False -> val_loss=16557.9340\n",
            "[62/108] bs=32, opt=adamw, lr=3.0e-04, wd=0.0e+00, unfreeze_layer4=True -> val_loss=8535.1168\n",
            "[63/108] bs=32, opt=adamw, lr=3.0e-04, wd=1.0e-05, unfreeze_layer4=False -> val_loss=16527.6966\n",
            "[64/108] bs=32, opt=adamw, lr=3.0e-04, wd=1.0e-05, unfreeze_layer4=True -> val_loss=5932.7542\n",
            "[65/108] bs=32, opt=adamw, lr=3.0e-04, wd=1.0e-04, unfreeze_layer4=False -> val_loss=17189.7418\n",
            "[66/108] bs=32, opt=adamw, lr=3.0e-04, wd=1.0e-04, unfreeze_layer4=True -> val_loss=6555.0723\n",
            "[67/108] bs=32, opt=adamw, lr=1.0e-03, wd=0.0e+00, unfreeze_layer4=False -> val_loss=16967.2825\n",
            "[68/108] bs=32, opt=adamw, lr=1.0e-03, wd=0.0e+00, unfreeze_layer4=True -> val_loss=2376.3801\n",
            "[69/108] bs=32, opt=adamw, lr=1.0e-03, wd=1.0e-05, unfreeze_layer4=False -> val_loss=16893.0504\n",
            "[70/108] bs=32, opt=adamw, lr=1.0e-03, wd=1.0e-05, unfreeze_layer4=True -> val_loss=3964.5486\n",
            "[71/108] bs=32, opt=adamw, lr=1.0e-03, wd=1.0e-04, unfreeze_layer4=False -> val_loss=16204.2672\n",
            "[72/108] bs=32, opt=adamw, lr=1.0e-03, wd=1.0e-04, unfreeze_layer4=True -> val_loss=4256.3623\n",
            "[73/108] bs=64, opt=adam, lr=1.0e-04, wd=0.0e+00, unfreeze_layer4=False -> val_loss=17089.4517\n",
            "[74/108] bs=64, opt=adam, lr=1.0e-04, wd=0.0e+00, unfreeze_layer4=True -> val_loss=9307.2364\n",
            "[75/108] bs=64, opt=adam, lr=1.0e-04, wd=1.0e-05, unfreeze_layer4=False -> val_loss=18132.0417\n",
            "[76/108] bs=64, opt=adam, lr=1.0e-04, wd=1.0e-05, unfreeze_layer4=True -> val_loss=16260.2425\n",
            "[77/108] bs=64, opt=adam, lr=1.0e-04, wd=1.0e-04, unfreeze_layer4=False -> val_loss=17014.0717\n",
            "[78/108] bs=64, opt=adam, lr=1.0e-04, wd=1.0e-04, unfreeze_layer4=True -> val_loss=15973.7963\n",
            "[79/108] bs=64, opt=adam, lr=3.0e-04, wd=0.0e+00, unfreeze_layer4=False -> val_loss=17355.0882\n",
            "[80/108] bs=64, opt=adam, lr=3.0e-04, wd=0.0e+00, unfreeze_layer4=True -> val_loss=10253.3801\n",
            "[81/108] bs=64, opt=adam, lr=3.0e-04, wd=1.0e-05, unfreeze_layer4=False -> val_loss=16757.2994\n",
            "[82/108] bs=64, opt=adam, lr=3.0e-04, wd=1.0e-05, unfreeze_layer4=True -> val_loss=7370.2399\n",
            "[83/108] bs=64, opt=adam, lr=3.0e-04, wd=1.0e-04, unfreeze_layer4=False -> val_loss=17710.1321\n",
            "[84/108] bs=64, opt=adam, lr=3.0e-04, wd=1.0e-04, unfreeze_layer4=True -> val_loss=12194.7662\n",
            "[85/108] bs=64, opt=adam, lr=1.0e-03, wd=0.0e+00, unfreeze_layer4=False -> val_loss=16445.9659\n",
            "[86/108] bs=64, opt=adam, lr=1.0e-03, wd=0.0e+00, unfreeze_layer4=True -> val_loss=16059.2813\n",
            "[87/108] bs=64, opt=adam, lr=1.0e-03, wd=1.0e-05, unfreeze_layer4=False -> val_loss=17321.0131\n",
            "[88/108] bs=64, opt=adam, lr=1.0e-03, wd=1.0e-05, unfreeze_layer4=True -> val_loss=71330.7045\n",
            "[89/108] bs=64, opt=adam, lr=1.0e-03, wd=1.0e-04, unfreeze_layer4=False -> val_loss=16060.3360\n",
            "[90/108] bs=64, opt=adam, lr=1.0e-03, wd=1.0e-04, unfreeze_layer4=True -> val_loss=3649.0631\n",
            "[91/108] bs=64, opt=adamw, lr=1.0e-04, wd=0.0e+00, unfreeze_layer4=False -> val_loss=17284.5444\n",
            "[92/108] bs=64, opt=adamw, lr=1.0e-04, wd=0.0e+00, unfreeze_layer4=True -> val_loss=14059.5555\n",
            "[93/108] bs=64, opt=adamw, lr=1.0e-04, wd=1.0e-05, unfreeze_layer4=False -> val_loss=17774.0117\n",
            "[94/108] bs=64, opt=adamw, lr=1.0e-04, wd=1.0e-05, unfreeze_layer4=True -> val_loss=11846.6278\n",
            "[95/108] bs=64, opt=adamw, lr=1.0e-04, wd=1.0e-04, unfreeze_layer4=False -> val_loss=17001.3482\n",
            "[96/108] bs=64, opt=adamw, lr=1.0e-04, wd=1.0e-04, unfreeze_layer4=True -> val_loss=16580.1097\n",
            "[97/108] bs=64, opt=adamw, lr=3.0e-04, wd=0.0e+00, unfreeze_layer4=False -> val_loss=16786.1855\n",
            "[98/108] bs=64, opt=adamw, lr=3.0e-04, wd=0.0e+00, unfreeze_layer4=True -> val_loss=14206.7871\n",
            "[99/108] bs=64, opt=adamw, lr=3.0e-04, wd=1.0e-05, unfreeze_layer4=False -> val_loss=18094.5701\n",
            "[100/108] bs=64, opt=adamw, lr=3.0e-04, wd=1.0e-05, unfreeze_layer4=True -> val_loss=6070.0178\n",
            "[101/108] bs=64, opt=adamw, lr=3.0e-04, wd=1.0e-04, unfreeze_layer4=False -> val_loss=17017.6351\n",
            "[102/108] bs=64, opt=adamw, lr=3.0e-04, wd=1.0e-04, unfreeze_layer4=True -> val_loss=17629.4134\n",
            "[103/108] bs=64, opt=adamw, lr=1.0e-03, wd=0.0e+00, unfreeze_layer4=False -> val_loss=18214.7823\n",
            "[104/108] bs=64, opt=adamw, lr=1.0e-03, wd=0.0e+00, unfreeze_layer4=True -> val_loss=15012.0967\n",
            "[105/108] bs=64, opt=adamw, lr=1.0e-03, wd=1.0e-05, unfreeze_layer4=False -> val_loss=16630.3247\n",
            "[106/108] bs=64, opt=adamw, lr=1.0e-03, wd=1.0e-05, unfreeze_layer4=True -> val_loss=8391.6101\n",
            "[107/108] bs=64, opt=adamw, lr=1.0e-03, wd=1.0e-04, unfreeze_layer4=False -> val_loss=16541.6151\n",
            "[108/108] bs=64, opt=adamw, lr=1.0e-03, wd=1.0e-04, unfreeze_layer4=True -> val_loss=121485.9609\n",
            "\n",
            "BEST CONFIG: {'batch_size': 16, 'optimizer': 'adamw', 'lr': 0.001, 'weight_decay': 1e-05, 'unfreeze_last_block': True} best_val_loss= 322.5186\n",
            "\n",
            "Using for full training (Chunk 14): batch_size=16, optimizer=adamw, lr=0.001, wd=1e-05, unfreeze_layer4=True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a58f510",
        "outputId": "466f3e02-4c81-4d8a-8a70-5b2965575b49"
      },
      "source": [
        "num_epochs = 20\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "\n",
        "    for images, ages in train_loader:\n",
        "        images = images.to(device)\n",
        "        ages = ages.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images).squeeze(1)  # (B, 1) -> (B,)\n",
        "        loss = criterion(outputs, ages)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_train_loss = running_train_loss / len(train_dataset)\n",
        "    train_losses.append(epoch_train_loss)\n",
        "\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, ages in val_loader:\n",
        "            images = images.to(device)\n",
        "            ages = ages.to(device)\n",
        "\n",
        "            outputs = model(images).squeeze(1)\n",
        "            loss = criterion(outputs, ages)\n",
        "            running_val_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_val_loss = running_val_loss / len(val_dataset)\n",
        "    val_losses.append(epoch_val_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Train Loss: 263.9415, Val Loss: 33604.1626\n",
            "Epoch [2/20], Train Loss: 171.0272, Val Loss: 4884.1188\n",
            "Epoch [3/20], Train Loss: 154.0092, Val Loss: 15335.4829\n",
            "Epoch [4/20], Train Loss: 147.8708, Val Loss: 1408.0562\n",
            "Epoch [5/20], Train Loss: 143.9877, Val Loss: 1406.2305\n",
            "Epoch [6/20], Train Loss: 127.7858, Val Loss: 4248.5435\n",
            "Epoch [7/20], Train Loss: 115.5750, Val Loss: 4778.6291\n",
            "Epoch [8/20], Train Loss: 129.3686, Val Loss: 7060.9810\n",
            "Epoch [9/20], Train Loss: 105.7318, Val Loss: 2434.5480\n",
            "Epoch [10/20], Train Loss: 115.1938, Val Loss: 1069.6129\n",
            "Epoch [11/20], Train Loss: 103.3354, Val Loss: 1345.4269\n",
            "Epoch [12/20], Train Loss: 98.3235, Val Loss: 292.9573\n",
            "Epoch [13/20], Train Loss: 109.2740, Val Loss: 278.4794\n",
            "Epoch [14/20], Train Loss: 115.1618, Val Loss: 252.5244\n",
            "Epoch [15/20], Train Loss: 92.4662, Val Loss: 1399.1960\n",
            "Epoch [16/20], Train Loss: 92.2723, Val Loss: 399.4397\n",
            "Epoch [17/20], Train Loss: 91.0654, Val Loss: 376.3563\n",
            "Epoch [18/20], Train Loss: 84.5216, Val Loss: 763.0282\n",
            "Epoch [19/20], Train Loss: 96.2697, Val Loss: 256.3689\n",
            "Epoch [20/20], Train Loss: 92.9242, Val Loss: 267.1410\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ff76c3c"
      },
      "source": [
        "## Evaluate Model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained model on the validation set and report key regression metrics such as Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a02eb86"
      },
      "source": [
        "**Reasoning**:\n",
        "To evaluate the model, I will import the necessary metrics from `sklearn.metrics` and then calculate and print the Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) using the collected validation predictions and actual ages.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e89b5f1",
        "outputId": "d6c10344-02ba-4f73-d83b-b6bf1073b595"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# This cell gives a more complete evaluation than just MAE/RMSE by adding:\n",
        "# - R^2: how much variance in age your model explains (0 = predicts mean; 1 = perfect; negative = worse than mean)\n",
        "# - Pearson correlation (r): whether predictions track true ages (trend alignment)\n",
        "# - Bias (mean error): whether you systematically over- or under-predict age\n",
        "# - Std of error: how spread out your errors are\n",
        "# - MedianAE + IQR: robust error stats (less sensitive to outliers)\n",
        "# - % within ±5/±10/±15 years: \"accuracy bands\" that are easier to interpret\n",
        "# - Worst 10 errors: sanity-check failure cases\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Collect validation predictions (fresh)\n",
        "# ----------------------------\n",
        "model.eval()\n",
        "y_true, y_pred = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, ages in val_loader:\n",
        "        images = images.to(device)\n",
        "        ages = ages.to(device)\n",
        "        preds = model(images).squeeze(1)\n",
        "\n",
        "        y_true.extend(ages.detach().cpu().numpy().tolist())\n",
        "        y_pred.extend(preds.detach().cpu().numpy().tolist())\n",
        "\n",
        "y_true = np.array(y_true, dtype=np.float32)\n",
        "y_pred = np.array(y_pred, dtype=np.float32)\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Core error arrays\n",
        "# ----------------------------\n",
        "err = y_pred - y_true           # signed error: + means over-predicting age\n",
        "abs_err = np.abs(err)\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Standard regression metrics\n",
        "# ----------------------------\n",
        "mae = mean_absolute_error(y_true, y_pred)                          # average absolute error (years)\n",
        "rmse = np.sqrt(mean_squared_error(y_true, y_pred))                 # penalizes large errors more than MAE\n",
        "r2 = r2_score(y_true, y_pred)                                      # variance explained (can be negative)\n",
        "\n",
        "# MAPE (percent error) can explode if ages near 0; not a concern here (adult ages), but we still guard just in case\n",
        "eps = 1e-6\n",
        "mape = float(np.mean(abs_err / np.maximum(np.abs(y_true), eps)) * 100.0)  # percent error\n",
        "\n",
        "# Pearson correlation: trend alignment between y_true and y_pred (1 = perfect increasing relationship)\n",
        "# If either array has ~0 variance, corr is undefined; we guard for that.\n",
        "if np.std(y_true) < 1e-8 or np.std(y_pred) < 1e-8:\n",
        "    pearson_r = float(\"nan\")\n",
        "else:\n",
        "    pearson_r = float(np.corrcoef(y_true, y_pred)[0, 1])\n",
        "\n",
        "# ----------------------------\n",
        "# 4) Calibration / bias and robust summaries\n",
        "# ----------------------------\n",
        "bias = float(np.mean(err))                     # mean signed error (years); negative = under-predicting\n",
        "std_err = float(np.std(err))                   # spread of signed errors\n",
        "median_ae = float(np.median(abs_err))          # robust \"typical\" absolute error\n",
        "q25, q75 = np.percentile(abs_err, [25, 75])\n",
        "iqr_ae = float(q75 - q25)                      # robust spread of absolute error\n",
        "\n",
        "# \"Within X years\" bands (easy to interpret)\n",
        "within_5 = float(np.mean(abs_err <= 5) * 100.0)\n",
        "within_10 = float(np.mean(abs_err <= 10) * 100.0)\n",
        "within_15 = float(np.mean(abs_err <= 15) * 100.0)\n",
        "\n",
        "# ----------------------------\n",
        "# 5) Worst-case errors (sanity check)\n",
        "# ----------------------------\n",
        "worst_k = 10\n",
        "worst_idx = np.argsort(-abs_err)[:worst_k]  # indices of largest absolute errors\n",
        "\n",
        "# ----------------------------\n",
        "# 6) Print a compact report\n",
        "# ----------------------------\n",
        "print(\"=== Validation Evaluation (More Than MAE/RMSE) ===\")\n",
        "print(f\"MAE:   {mae:.4f} years  (avg absolute error)\")\n",
        "print(f\"RMSE:  {rmse:.4f} years  (penalizes big misses)\")\n",
        "print(f\"R^2:   {r2:.4f}         (variance explained; <0 = worse than predicting mean age)\")\n",
        "print(f\"r:     {pearson_r:.4f}   (Pearson correlation; trend alignment)\")\n",
        "print(f\"MAPE:  {mape:.2f}%      (average percent error)\")\n",
        "print()\n",
        "print(f\"Bias (mean error): {bias:+.4f} years  (+ = overpredict, - = underpredict)\")\n",
        "print(f\"Std of error:      {std_err:.4f} years (spread of signed errors)\")\n",
        "print(f\"MedianAE:          {median_ae:.4f} years (robust typical error)\")\n",
        "print(f\"IQR(|error|):      {iqr_ae:.4f} years (robust spread of abs error)\")\n",
        "print()\n",
        "print(f\"% within ±5 years:  {within_5:.2f}%\")\n",
        "print(f\"% within ±10 years: {within_10:.2f}%\")\n",
        "print(f\"% within ±15 years: {within_15:.2f}%\")\n",
        "print()\n",
        "print(f\"Worst {worst_k} absolute errors (true -> pred | abs_err):\")\n",
        "for i in worst_idx:\n",
        "    print(f\"  {y_true[i]:6.2f} -> {y_pred[i]:6.2f} | {abs_err[i]:6.2f}\")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Validation Evaluation (More Than MAE/RMSE) ===\n",
            "MAE:   14.3972 years  (avg absolute error)\n",
            "RMSE:  16.3444 years  (penalizes big misses)\n",
            "R^2:   0.0685         (variance explained; <0 = worse than predicting mean age)\n",
            "r:     0.2980   (Pearson correlation; trend alignment)\n",
            "MAPE:  36.34%      (average percent error)\n",
            "\n",
            "Bias (mean error): +1.7107 years  (+ = overpredict, - = underpredict)\n",
            "Std of error:      16.2547 years (spread of signed errors)\n",
            "MedianAE:          15.3951 years (robust typical error)\n",
            "IQR(|error|):      11.9121 years (robust spread of abs error)\n",
            "\n",
            "% within ±5 years:  12.50%\n",
            "% within ±10 years: 30.68%\n",
            "% within ±15 years: 48.86%\n",
            "\n",
            "Worst 10 absolute errors (true -> pred | abs_err):\n",
            "   28.48 ->  60.26 |  31.79\n",
            "   25.45 ->  56.50 |  31.05\n",
            "   80.17 ->  50.17 |  30.00\n",
            "   75.94 ->  47.67 |  28.27\n",
            "   30.18 ->  57.67 |  27.48\n",
            "   22.65 ->  49.90 |  27.25\n",
            "   22.97 ->  49.64 |  26.67\n",
            "   24.90 ->  49.93 |  25.03\n",
            "   24.85 ->  49.44 |  24.58\n",
            "   21.57 ->  44.55 |  22.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be43992c"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the model's performance and discuss any initial insights gained from the age prediction task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f68c163"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "\n",
        "*   **Model's performance:** The model achieved a Mean Absolute Error (MAE) of 7.1816 and a Root Mean Squared Error (RMSE) of 9.0908 on the validation set for age prediction.\n",
        "*   **Initial insights gained:**\n",
        "    *   The model can learn to predict age from MRI images, albeit with a notable error margin. An MAE of around 7 years suggests predictions are typically off by about 7 years.\n",
        "    *   Careful handling of data transformations, especially channel consistency (converting grayscale MRI images to a 3-channel format expected by pre-trained models like ResNet50), is crucial for successful deep learning image tasks.\n",
        "    *   The `SettingWithCopyWarning` was successfully mitigated by explicitly copying the DataFrame before modification.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Data Path Preparation:** A new `image_path` column was successfully added to the DataFrame, providing full file paths for image loading. The `SettingWithCopyWarning` was resolved by explicitly copying the DataFrame.\n",
        "*   **Data Splitting:** The dataset was split into a training set of 470 samples and a validation set of 118 samples, with 80% for training and 20% for validation.\n",
        "*   **Custom Dataset and DataLoader:** A custom `MRIImageDataset` was created to handle image loading, transformations, and age label retrieval. `DataLoader` instances were successfully created for both training (470 samples) and validation (118 samples) with a batch size of 32.\n",
        "*   **Model Architecture:** A pre-trained ResNet50 model was loaded, and its final fully connected layer was successfully modified to output a single value for age regression.\n",
        "*   **Image Channel Handling:** To ensure compatibility with the 3-channel input expectation of ResNet50, a custom `GrayscaleToRGBTensor` transformation was implemented. This allowed 1-channel grayscale MRI images to be correctly processed by the model by replicating the single channel three times.\n",
        "*   **Loss Function and Optimizer:** Mean Squared Error (MSE) was set as the loss function for regression, and the Adam optimizer with a learning rate of 0.001 was chosen for model training.\n",
        "*   **Model Training:** The model was successfully trained for 20 epochs. The training process encountered and resolved an initial channel mismatch error by incorporating the custom 3-channel conversion into the data transformation pipeline.\n",
        "*   **Model Evaluation:** On the validation set, the model achieved a Mean Absolute Error (MAE) of 7.1816 and a Root Mean Squared Error (RMSE) of 9.0908.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current MAE of ~7 years suggests room for improvement. Further optimization could include hyperparameter tuning (learning rate, batch size, optimizer), experimenting with different data augmentations, or exploring more complex model architectures (e.g., deeper networks or specialized MRI models).\n",
        "*   Consider performing error analysis on the validation set predictions to understand if the model consistently mispredicts certain age groups or image characteristics, which could guide further data preprocessing or model refinement.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}